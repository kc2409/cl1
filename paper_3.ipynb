{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP96XX/7X/fgPVF1UMaBQIa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kc2409/cl1/blob/main/paper_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install logger"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enm9dLcDXqEL",
        "outputId": "3ae787e2-d05e-4cdd-ffa2-577d0db26146"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting logger\n",
            "  Downloading logger-1.4.tar.gz (1.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: logger\n",
            "  Building wheel for logger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for logger: filename=logger-1.4-py3-none-any.whl size=1759 sha256=c21a0431e4b893575ad598d6cfe2f25600d964f570fb371fa7c486d5e4c5a5af\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/19/7b/09fc73f7503166eaf7f31b4aa0095b7f78af2ec0898e1f8312\n",
            "Successfully built logger\n",
            "Installing collected packages: logger\n",
            "Successfully installed logger-1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txbpDyu2YwlM",
        "outputId": "8f4d5e4e-43a8-40f6-fbfc-a5def16b1e2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jahAPFiU6rxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class unitedCLLoss(nn.Module):\n",
        "    def __init__(self, opt, contrast_mode='all'):\n",
        "        super(unitedCLLoss, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.temperature = opt.temperature\n",
        "        self.contrast_mode = contrast_mode\n",
        "\n",
        "    def forward(self, features, labels, mask=None):\n",
        "        \"\"\"\n",
        "            Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
        "            It also supports the unsupervised contrastive loss in SimCLR\n",
        "        \"\"\"\n",
        "        \"\"\" Compute loss for model. If both `labels` and `mask` are None,\n",
        "            it degenerates to SimCLR unsupervised loss:\n",
        "            https://arxiv.org/pdf/2002.05709.pdf\n",
        "            Args:\n",
        "                features: hidden vector of shape [bsz, n_views, ...].\n",
        "                labels: ground truth of shape [bsz].\n",
        "                mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
        "                    has the same class as sample i. Can be asymmetric.\n",
        "            Returns:\n",
        "                A loss scalar.\n",
        "        \"\"\"\n",
        "        device = (torch.device('cuda')\n",
        "                  if features.is_cuda\n",
        "                  else torch.device('cpu'))\n",
        "\n",
        "        if len(features.shape) < 3:\n",
        "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
        "                             'at least 3 dimensions are required')\n",
        "        if len(features.shape) > 3:\n",
        "            features = features.view(features.shape[0], features.shape[1], -1)\n",
        "\n",
        "        batch_size = features.shape[0]\n",
        "        if labels is not None and mask is not None:\n",
        "            raise ValueError('Cannot define both `labels` and `mask`')\n",
        "        elif labels is None and mask is None:\n",
        "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
        "        elif labels is not None:\n",
        "            labels = labels.contiguous().view(-1, 1)\n",
        "            if labels.shape[0] != batch_size:\n",
        "                labels = torch.cat([labels, labels], dim=0)\n",
        "\n",
        "            mask = torch.eq(labels, labels.T).float().add(0.0000001).to(\n",
        "                device)\n",
        "        else:\n",
        "            mask = mask.float().to(device)\n",
        "\n",
        "        contrast_count = features.shape[1]\n",
        "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
        "        if self.contrast_mode == 'one':\n",
        "            anchor_feature = features[:, 0]\n",
        "            anchor_count = 1\n",
        "        elif self.contrast_mode == 'all':\n",
        "            anchor_feature = contrast_feature\n",
        "            anchor_count = contrast_count\n",
        "        else:\n",
        "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
        "\n",
        "        # tile mask\n",
        "        mask = mask.repeat(anchor_count, contrast_count)\n",
        "\n",
        "        # mask-out self-contrast cases\n",
        "        logits_mask = torch.scatter(\n",
        "            torch.ones_like(mask),\n",
        "            1,\n",
        "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
        "            0\n",
        "        )\n",
        "        mask_pos = mask * logits_mask\n",
        "        mask_neg = (torch.ones_like(mask) - mask) * logits_mask\n",
        "\n",
        "        similarity = torch.exp(torch.mm(anchor_feature, contrast_feature.t()) / self.temperature)\n",
        "\n",
        "        pos = torch.sum(similarity * mask_pos, 1)\n",
        "        neg = torch.sum(similarity * mask_neg, 1)\n",
        "        loss = -(torch.mean(torch.log(pos / (pos + neg))))\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "mTozMxYjYkPj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "#from criterion import unitedCLLoss\n",
        "#from logger.CSVlogger import CSVlogger\n",
        "#from model import SSCL\n",
        "from sklearn import metrics\n",
        "#from utils.data_utils import DatesetReader\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "1fxDbRyhn6e6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertConfig, AutoTokenizer"
      ],
      "metadata": {
        "id": "bkG3m5IWYp_B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'data': torch.tensor(self.data[idx], dtype=torch.float32),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "        return sample"
      ],
      "metadata": {
        "id": "84XEXG0KZHxr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1QsYQwmh0ah",
        "outputId": "174462a3-8028-43bc-a76f-25431aa5e337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from torchtext.legacy import data\n",
        "\n",
        "\n",
        "df = pd.read_excel('la_train.xlsx')\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx]\n",
        "        text = sample['text']\n",
        "        sentiment_label = sample['sentiment_label']\n",
        "        stance_label = sample['stance_label']\n",
        "        return {\n",
        "            'text': text,\n",
        "            'sentiment_label': sentiment_label,\n",
        "            'stance_label': stance_label\n",
        "        }\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "dataset = CustomDataset(df)\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hmHYaaLAhwnZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H1hrOTsiJsk",
        "outputId": "c0c46b7c-40b8-474c-e6cb-233277925d1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x79901493b130>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Define the maximum sequence length\n",
        "max_sequence_length = 24\n",
        "\n",
        "# Text Encoder architecture\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        return lstm_out[:, -1, :]\n",
        "\n",
        "# Projection Head architecture\n",
        "# Projection Head architecture\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ProjectionHead, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# Custom Contrastive Loss\n",
        "class CustomContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super(CustomContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        z1 = z1 / torch.norm(z1, dim=1, keepdim=True)\n",
        "        z2 = z2 / torch.norm(z2, dim=1, keepdim=True)\n",
        "        sim = torch.mm(z1, z2.t())\n",
        "        sim /= self.temperature\n",
        "        loss = torch.nn.functional.cross_entropy(sim, torch.arange(len(sim)))\n",
        "        return loss\n",
        "\n",
        "# Stance Model combining Text Encoder and Projection Head\n",
        "class StanceModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, projection_dim):\n",
        "        super(StanceModel, self).__init__()\n",
        "        self.text_encoder = TextEncoder(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
        "        self.projection_head = ProjectionHead(hidden_dim, projection_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        text_representation = self.text_encoder(text)\n",
        "        projected_representation = self.projection_head(text_representation)\n",
        "        return projected_representation\n",
        "\n",
        "# Load your dataset (replace 'la_train.xlsx' with your dataset file)\n",
        "data = pd.read_excel('la_train.xlsx')\n",
        "\n",
        "# Extract relevant data columns\n",
        "text_data = data['text']\n",
        "stance_labels = data['stance_label']\n",
        "\n",
        "# Tokenize your text data (assuming you have a tokenizer)\n",
        "# Replace this with your actual tokenizer or tokenization process\n",
        "tokenized_texts = [text.split() for text in text_data]\n",
        "\n",
        "# Create a vocabulary and mapping for word to index\n",
        "vocab = set(word for text in tokenized_texts for word in text)\n",
        "vocab_size = len(vocab)\n",
        "vocab_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Encode stance labels\n",
        "label_encoder = LabelEncoder()\n",
        "stance_labels = label_encoder.fit_transform(stance_labels)\n",
        "\n",
        "# Convert text sequences to lists of indices\n",
        "text_sequences = [[vocab_to_index[word] for word in text] for text in tokenized_texts]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_sequences, stance_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Pad the text sequences to a consistent length\n",
        "padded_sequences = [torch.LongTensor(seq[:max_sequence_length]) if len(seq) >= max_sequence_length\n",
        "                    else torch.cat((torch.LongTensor(seq), torch.zeros(max_sequence_length - len(seq), dtype=torch.long)))\n",
        "                    for seq in X_train]\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(torch.stack(padded_sequences), torch.tensor(y_train, dtype=torch.long))\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define model hyperparameters\n",
        "embedding_dim = 128\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "projection_dim = 32\n",
        "temperature = 0.5\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Instantiate the StanceModel, optimizer, and contrastive loss\n",
        "model = StanceModel(vocab_size, embedding_dim, hidden_dim, num_layers, projection_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "contrastive_loss = CustomContrastiveLoss(temperature=temperature)\n",
        "\n",
        "# Move the model to a GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids_batch, stance_batch = [item.to(device) for item in batch]\n",
        "\n",
        "        output = model(input_ids_batch)\n",
        "\n",
        "        loss = contrastive_loss(output, output)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}] Loss: {avg_loss:.4f}')\n",
        "\n",
        "print('Training complete!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHm48t8ZuYOm",
        "outputId": "9db6aad4-f1ed-4e25-d902-9a46a6cfc74d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 50/50 [00:02<00:00, 20.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 2.9607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 50/50 [00:02<00:00, 21.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10] Loss: 2.5078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 50/50 [00:03<00:00, 16.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10] Loss: 2.4213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 50/50 [00:02<00:00, 17.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10] Loss: 2.3949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 50/50 [00:02<00:00, 22.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10] Loss: 2.3733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 50/50 [00:02<00:00, 22.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10] Loss: 2.3639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 50/50 [00:02<00:00, 22.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10] Loss: 2.3482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 50/50 [00:02<00:00, 22.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10] Loss: 2.3411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 50/50 [00:02<00:00, 17.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10] Loss: 2.3265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 50/50 [00:03<00:00, 14.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10] Loss: 2.3218\n",
            "Training complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnSVYEYpuaAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}