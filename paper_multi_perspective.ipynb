{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSjrhr/0tRO+XeABCc+6WT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kc2409/cl1/blob/main/paper_multi_perspective.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOvjLd5ZEZ3h",
        "outputId": "6bc19368-fc5c-4de4-bb91-b31c857ced22"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4teQqwnG5hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdrC0cVHRTuc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "max_seq_length = 128\n",
        "\n",
        "\n",
        "class StanceDetectionModel(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(StanceDetectionModel, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.stance_classifier = nn.Linear(bert_model.config.hidden_size, num_stance_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs[0]\n",
        "        logits = self.stance_classifier(last_hidden_state[:, 0, :])\n",
        "        return logits\n",
        "\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "stance_model = StanceDetectionModel(bert_model)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(stance_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    stance_model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        logits = stance_model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    #\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Inference\n",
        "#stance_model.eval()\n",
        "#with torch.no_grad():\n",
        "#    for batch in test_dataloader:\n",
        "#        input_ids, attention_mask = batch\n",
        "#        logits = stance_model(input_ids, attention_mask)\n",
        "#        predicted_labels = torch.argmax(logits, dim=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the projection layers\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ProjectionLayer, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Define the Target-oriented contrastive loss\n",
        "class TargetContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature):\n",
        "        super(TargetContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        # Calculate the contrastive loss for target\n",
        "        target_similarity = torch.nn.functional.cosine_similarity(anchor, positive, dim=-1) / self.temperature\n",
        "        loss = -torch.log(torch.exp(target_similarity).sum() / (torch.exp(target_similarity).sum() + torch.exp(negative).sum()))\n",
        "        return loss\n",
        "\n",
        "# Define the Label-oriented contrastive loss\n",
        "class LabelContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature):\n",
        "        super(LabelContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        # Calculate the contrastive loss for label\n",
        "        label_similarity = torch.nn.functional.cosine_similarity(anchor, positive, dim=-1) / self.temperature\n",
        "        loss = -torch.log(torch.exp(label_similarity).sum() / (torch.exp(label_similarity).sum() + torch.exp(negative).sum()))\n",
        "        return loss\n",
        "\n",
        "# Define your StanceModel class\n",
        "class StanceModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, temperature):\n",
        "        super(StanceModel, self).__init__()  # Add parentheses here\n",
        "        self.target_projection = ProjectionLayer(input_dim, hidden_dim, output_dim)\n",
        "        self.text_projection = ProjectionLayer(input_dim, hidden_dim, output_dim)\n",
        "        self.label_projection = ProjectionLayer(input_dim, hidden_dim, output_dim)\n",
        "        self.target_contrastive_loss = TargetContrastiveLoss(temperature)\n",
        "        self.label_contrastive_loss = LabelContrastiveLoss(temperature)\n",
        "\n",
        "    def forward(self, target, text, label):\n",
        "        target_projection = self.target_projection(target)\n",
        "        text_projection = self.text_projection(text)\n",
        "        label_projection = self.label_projection(label)\n",
        "\n",
        "        return target_projection, text_projection, label_projection\n",
        "\n",
        "input_dim = 300  # If you're using 300-dimensional word embeddings (example value)\n",
        "hidden_dim = 256  # Adjust based on your model's complexity (example value)\n",
        "output_dim = 3  # Since it appears you have three classes in your 'stance_label' column\n",
        "temperature = 1.0  # A common starting value for contrastive learning\n",
        "\n",
        "# Instantiate the StanceModel with these hyperparameters\n",
        "model = StanceModel(input_dim, hidden_dim, output_dim, temperature)\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the StanceModel, optimizer, and contrastive losses\n",
        "model = StanceModel(input_dim, hidden_dim, output_dim, temperature)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        targets, texts, labels = batch\n",
        "\n",
        "        # Forward pass\n",
        "        target_proj, text_proj, label_proj = model(targets, texts, labels)\n",
        "\n",
        "        # Calculate target contrastive loss and label contrastive loss\n",
        "        target_loss = model.target_contrastive_loss(target_proj, text_proj, label_proj)\n",
        "        label_loss = model.label_contrastive_loss(label_proj, text_proj, target_proj)\n",
        "\n",
        "        # Calculate the overall loss\n",
        "        total_loss = alpha * target_loss + beta * label_loss\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Stance classification\n",
        "# You can add your stance classification component here based on target_proj, text_proj, and label_proj\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "T2tya9XmG6Rk",
        "outputId": "1506900b-ceb5-475a-a63b-ce8f330b81d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dce691bacda7>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the projection layers\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ProjectionLayer, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Define the Target-oriented contrastive loss\n",
        "class TargetContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature):\n",
        "        super(TargetContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        # Calculate the contrastive loss for target\n",
        "        target_similarity = torch.nn.functional.cosine_similarity(anchor, positive, dim=-1) / self.temperature\n",
        "        loss = -torch.log(torch.exp(target_similarity).sum() / (torch.exp(target_similarity).sum() + torch.exp(negative).sum()))\n",
        "        return loss\n",
        "\n",
        "# Define the Label-oriented contrastive loss\n",
        "class LabelContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature):\n",
        "        super(LabelContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        # Calculate the contrastive loss for label\n",
        "        label_similarity = torch.nn.functional.cosine_similarity(anchor, positive, dim=-1) / self.temperature\n",
        "        loss = -torch.log(torch.exp(label_similarity).sum() / (torch.exp(label_similarity).sum() + torch.exp(negative).sum()))\n",
        "        return loss\n",
        "\n",
        "# Define your StanceModel class\n",
        "class StanceModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, temperature):\n",
        "        super(StanceModel, self).__init__()\n",
        "        self.target_projection = ProjectionLayer(input_dim, hidden_dim, output_dim)\n",
        "        self.text_projection = ProjectionLayer(input_dim, hidden_dim, output_dim)\n",
        "        self.label_projection = ProjectionLayer(input_dim, hidden_dim, output_dim)\n",
        "        self.target_contrastive_loss = TargetContrastiveLoss(temperature)\n",
        "        self.label_contrastive_loss = LabelContrastiveLoss(temperature)\n",
        "\n",
        "    def forward(self, target, text, label):\n",
        "        target_projection = self.target_projection(target)\n",
        "        text_projection = self.text_projection(text)\n",
        "        label_projection = self.label_projection(label)\n",
        "\n",
        "        return target_projection, text_projection, label_projection\n",
        "\n",
        "# Instantiate the StanceModel, optimizer, and contrastive losses\n",
        "input_dim = 128  # Replace with the actual input dimension\n",
        "hidden_dim = 64  # Replace with your desired hidden dimension\n",
        "output_dim = 32  # Replace with your desired output dimension\n",
        "temperature = 0.07  # Replace with your desired temperature\n",
        "\n",
        "model = StanceModel(input_dim, hidden_dim, output_dim, temperature)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Example data (replace with your actual data)\n",
        "targets_data = torch.randn(100, input_dim)  # Replace with your target data\n",
        "texts_data = torch.randn(100, input_dim)  # Replace with your text data\n",
        "labels_data = torch.randn(100, input_dim)  # Replace with your label data\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset = TensorDataset(targets_data, texts_data, labels_data)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32  # Adjust the batch size as needed\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Adjust the number of epochs as needed\n",
        "for epoch in range(num_epochs):\n",
        "     total_loss = 0.0\n",
        "     correct = 0\n",
        "     total = 0\n",
        "     for batch in dataloader:\n",
        "          targets, texts, labels = batch\n",
        "\n",
        "          # Forward pass\n",
        "          target_proj, text_proj, label_proj = model(targets, texts, labels)\n",
        "\n",
        "          # Calculate target contrastive loss and label contrastive loss\n",
        "          target_loss = model.target_contrastive_loss(target_proj, text_proj, label_proj)\n",
        "          label_loss = model.label_contrastive_loss(label_proj, text_proj, target_proj)\n",
        "\n",
        "          # Calculate the overall loss\n",
        "          alpha = 0.5  # Adjust these values as needed\n",
        "          beta = 0.5\n",
        "          total_loss = alpha * target_loss + beta * label_loss\n",
        "\n",
        "          # Backpropagation and optimization\n",
        "          optimizer.zero_grad()\n",
        "          total_loss.backward()\n",
        "          optimizer.step()\n",
        "          # _, predicted = target_proj.max(1)\n",
        "          # total += labels.size(0)\n",
        "          # correct += (predicted == labels).sum().item()\n",
        "\n",
        "          epoch_loss = total_loss / len(dataloader)\n",
        "          print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy5TALUFOtD4",
        "outputId": "96846e45-72e8-408a-e4c7-8c2e90a0ca4b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1423\n",
            "Epoch 1/10, Loss: 0.0160\n",
            "Epoch 1/10, Loss: 0.0034\n",
            "Epoch 1/10, Loss: 0.0034\n",
            "Epoch 2/10, Loss: 0.0007\n",
            "Epoch 2/10, Loss: 0.0006\n",
            "Epoch 2/10, Loss: 0.0008\n",
            "Epoch 2/10, Loss: 0.0004\n",
            "Epoch 3/10, Loss: 0.0007\n",
            "Epoch 3/10, Loss: 0.0011\n",
            "Epoch 3/10, Loss: 0.0006\n",
            "Epoch 3/10, Loss: 0.0004\n",
            "Epoch 4/10, Loss: 0.0006\n",
            "Epoch 4/10, Loss: 0.0007\n",
            "Epoch 4/10, Loss: 0.0006\n",
            "Epoch 4/10, Loss: 0.0041\n",
            "Epoch 5/10, Loss: 0.0008\n",
            "Epoch 5/10, Loss: 0.0008\n",
            "Epoch 5/10, Loss: 0.0003\n",
            "Epoch 5/10, Loss: 0.0001\n",
            "Epoch 6/10, Loss: 0.0004\n",
            "Epoch 6/10, Loss: 0.0005\n",
            "Epoch 6/10, Loss: 0.0006\n",
            "Epoch 6/10, Loss: 0.0002\n",
            "Epoch 7/10, Loss: 0.0005\n",
            "Epoch 7/10, Loss: 0.0003\n",
            "Epoch 7/10, Loss: 0.0003\n",
            "Epoch 7/10, Loss: 0.0028\n",
            "Epoch 8/10, Loss: 0.0004\n",
            "Epoch 8/10, Loss: 0.0004\n",
            "Epoch 8/10, Loss: 0.0003\n",
            "Epoch 8/10, Loss: 0.0002\n",
            "Epoch 9/10, Loss: 0.0002\n",
            "Epoch 9/10, Loss: 0.0002\n",
            "Epoch 9/10, Loss: 0.0005\n",
            "Epoch 9/10, Loss: 0.0001\n",
            "Epoch 10/10, Loss: 0.0001\n",
            "Epoch 10/10, Loss: 0.0003\n",
            "Epoch 10/10, Loss: 0.0004\n",
            "Epoch 10/10, Loss: 0.0002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "data = pd.read_excel('la_train.xlsx')\n",
        "\n",
        "text_inputs = data['text']\n",
        "labels = data['sentiment_label']\n",
        "\n",
        "\n",
        "train_text, val_text, train_labels, val_labels = train_test_split(text_inputs, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set shape - text:\", train_text.shape)\n",
        "print(\"Validation set shape - text:\", val_text.shape)\n",
        "print(\"Training set shape - labels:\", train_labels.shape)\n",
        "print(\"Validation set shape - labels:\", val_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M06kE0-YQ6Sd",
        "outputId": "c1572402-80ad-43de-f36d-5206a3f7bb89"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape - text: (3149,)\n",
            "Validation set shape - text: (788,)\n",
            "Training set shape - labels: (3149,)\n",
            "Validation set shape - labels: (788,)\n"
          ]
        }
      ]
    }
  ]
}